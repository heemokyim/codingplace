http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/
http://kysepark.blogspot.kr/2016/04/how-to-tune-your-apache-spark-jobs-part.html

* executor는 프로세스지 반드시 machine일 필요없다

* 고려해야 할 것
0. shuffle을 최소화시키는 action을 유도해라
1. --num-executor = executor 갯수
2. --executor-cores = executor 당 코어갯수
3. --executor-memory = executor 당 메모리할당량
4. partition이 모든 코어를 충분히 활용하도록 설계됬는가
    ( partition갯수 / 코어갯수의 나머지가 없도록 partitioning)
    ( 모든 partition에 data가 uniform distributed하게 해서 코어의 idle을 줄여야 함)
    ( 그럼 어떻게 나누는가?
        1. repartition = 코어와 데이터 수를 고려하여 partition함
        2. partitionBy = key를 쓰는 action에 최적화되게 partition함, e.g reduceByKey, join
        lazy evaluation이 자주 일어나는 RDD에 대해 쓰이므로 cache나 persist를 같이 쓰는 편이다)
            https://stackoverflow.com/questions/33831561/pyspark-repartition-vs-partitionby
    
5. 데이터가 너무 커서 executor에 할당된 메모리가 부족한지 확인
    (이럴 경우 partition을 많이 시켜라)


* 클러스터 실행예제
$ spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 1 \
    --queue thequeue \
    examples/jars/spark-examples*.jar \
    10
    

$ spark-submit
    --driver-memory 100g
    --num-executors 60
    --conf 'spark.executor.memory=20g'
    --conf 'yarn.nodemanager.resource.memory-mb=249g'
    --conf 'yarn.nodemanager.resource.cpu-vcores=31'
    code.py


* executor configuration은 어떻게 해야 하나
ex)

1.
클러스터 -> 노드 3개
노드당 코어갯수 -> 32개
노드당 메모리 -> 250GB

2.
총 executor 갯수를 각 노드에 같은 갯수로 퍼지게 정해라.
18개면 노드당 6개, executor당 core 5개를 할당할 수 있다
이 때 yarn am을 위해 executor는 하나 빼줘서 총 17개
** 주의점 = core 5개 이상 할당하면 성능떨어짐
https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory

3. 
executor당 메모리는 249 / 6 = 41 (내림)
executor당 메모리 오버헤드는 41 * 0.07(default) = 3(올림)

따라서 executor 당 메모리는 41 - 3 = 38G

*** 결론
-num-executor 17 --executor-core 5 --executor-memory 38G
= 총 18개의 executor, 각 노드당 6개의 executor
= 각 executor가 5개 core
= 각 executor가 38G memory