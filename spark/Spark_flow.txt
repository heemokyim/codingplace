1. Create RDD from SparkContext

2. Transform, Action on RDD

3. Lazy Evaluation

--------------------------------------

transform = map (정제)
action = reduce (결과도출)

sn3, hdfs, parallize

https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/

1. RDD.map(func) = 모든 행에 대해 transform을 수행 

2. RDD.reduce(func) = transformed된 행에 대해 action 수행

--------------------------------------

RDD.collect() = 현재 진행상황을 보여줘

RDD.map(func) = [A,B,C] -> [A',B',C']

RDD.flatMap(func) = [A,B,C] -> [D]

RDD.filter(func) = [A,2A,3A,4A] -> [2A,4A]

--------------------------------------

broadcast = shared data to be processed over clustser

accumulator = global varaible like signal over cluster

cache = cache result on memory

persist = store result on disk

--------------------------------------

User based CF -> 취향이 비슷한 유저가 산 걸 추천

Item based CF -> 비슷한 아이템을 추천 (비슷한 걸 계산하는 기준 -> 코사인 유사도 + 평점)

** 추천 시스템 유사도 계산하는 두가지 방법
1. 두 개씩(유저,아이템) 짝지어서 [[first,second],[sim_score, numpairs]]로 표현
2. 유사도 행렬

--------------------------------------

sc.textFile('s3n://..')
= s3는 amazon simple data storage service다 (can be used as distributed file system)
= EMR이랑 같이 쓰기 좋음

--------------------------------------

Hadoop이 fault tolerance하긴 하지만
설계를 잘못해서 생긴 fault에 대해선 tolerated하지 못하다.
=> executor한테 얼마나 일을 줄지, 몇개나 쓸지, 메모리는 얼마나 줄 지 잘 결정해라

--------------------------------------

https://stackoverflow.com/questions/31460079/spark-ui-on-aws-emr

--------------------------------------s